{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FindingNouns.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "MWwZhFCLovmd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Create Model"
      ]
    },
    {
      "metadata": {
        "id": "Ulyyv9_xo7BD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import random\n",
        "from ClassifierBasedGermanTagger import ClassifierBasedGermanTagger\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "37pl23JZMEkm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "corp = nltk.corpus.ConllCorpusReader('.', 'tiger.conll09',\n",
        "                                     ['ignore', 'words', 'ignore', 'ignore', 'pos'],\n",
        "                                     encoding='utf-8')\n",
        "\n",
        "tagged_sents = corp.tagged_sents()\n",
        "\n",
        "# set a split size: use 90% for training, 10% for testing\n",
        "split_perc = 0.1\n",
        "split_size = int(len(tagged_sents) * split_perc)\n",
        "train_sents, test_sents = tagged_sents[split_size:], tagged_sents[:split_size]\n",
        "\n",
        "tagger = ClassifierBasedGermanTagger(train=train_sents)\n",
        "\n",
        "accuracy = tagger.evaluate(test_sents)\n",
        "print(\"ACCURACY: \" + accuracy)\n",
        "\n",
        "with open('nltk_german_classifier_data.pickle', 'wb') as f:\n",
        "    pickle.dump(tagger, f, protocol=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0yZfp166pNEa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load Model from File"
      ]
    },
    {
      "metadata": {
        "id": "Ik9LF3v0pbpx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import random\n",
        "import pickle\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from joblib import Parallel, delayed\n",
        "from nltk.tag import pos_tag\n",
        "from random import shuffle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fb0-Sfs4O5lz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def loadData():\n",
        "  with open('C:\\\\Users\\\\Adjek\\\\Nextcloud\\\\DeeptechAI\\\\websiteData\\\\websiteContent.pickle', 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "    return data\n",
        "  \n",
        "def tagData(text):\n",
        "  result = dict()\n",
        "  for i,j in zip(tqdm(range(len(text.keys()))),text.keys()):\n",
        "    tagged_sent = pos_tag(text[j].split())\n",
        "\n",
        "    propernouns = [word for word,pos in tagged_sent if ((pos == 'NNP') & (len(word) > 5) & (len(word) < 25))]\n",
        "    \n",
        "    propernouns = list(set(propernouns))\n",
        "    \n",
        "    filt = ['Webseite', 'Cookie']\n",
        "    propernouns = [k for k in propernouns if k not in filt]\n",
        "    \n",
        "    shuffle(propernouns)\n",
        "    \n",
        "    result[j] = propernouns[0:5:1]\n",
        "    \n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rjVGd44TPXld",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = loadData()\n",
        "taggedData = tagData(data)\n",
        "print(type(data))\n",
        "print(len(data))\n",
        "print(len(taggedData))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "94T4oHMn-B8h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(taggedData[\"www.astrazeneca.de\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WRUNm1L9R-mS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('taggedwords.pickle', 'wb') as f:\n",
        "    pickle.dump(taggedData, f, protocol=2)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}