# -*- coding: utf-8 -*-
"""WordExtraction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IT-sB8ajEJyR0gYeTKynFhIhG7JvtSqd
"""

#Install package "beautifulsoup4" with conda

import urllib.request
import pandas as pd 
import pickle
import time
import ssl
from bs4 import BeautifulSoup
from tqdm import tqdm

def readCsv():
  # From Roland
  data = pd.read_csv("C:\\Users\\Julian\\Nextcloud\\DeeptechAI\\data\\urls.csv", sep = ";", encoding = "ISO-8859-1")
  return data

def getListOfURLs(csvData):
  urlList = list()
  for url in csvData.Domain:
    urlList.append(url)
  return urlList

def getWebsiteData(url):
  websiteData = "<html><head></head><body></body></html>" #empty website
  try:
    websiteData = urllib.request.urlopen("https://" + url, timeout=2, context=ssl._create_unverified_context())
  except urllib.error.URLError as e:
    print("# URLError: Switching to http: " + url)
    try: 
      websiteData = urllib.request.urlopen("http://" + url, timeout=2, context=ssl._create_unverified_context())
    except Exception as e:
      print("HTTP and HTTPS call not possible, skipping website: " + url)
      print(str(e))
  except Exception as e:
    print("Unhandled Error, skipping website: " + url)
    print(str(e))
  return websiteData
  

def getWebsiteText(url):  
  print("Getting Website data from " + url)
  text = ""
  # Skip sites with error
  htmlContent =  getWebsiteData(url)
  soup = lambda: None
  try: 
    soup = BeautifulSoup(htmlContent)
    # kill all script and style elements
    for script in soup(["script", "style"]):
        script.extract()    # rip it out
    # get text
    text = soup.get_text()
    # break into lines and remove leading and trailing space on each
    lines = (line.strip() for line in text.splitlines())
    # break multi-headlines into a line each
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    # drop blank lines
    text = '\n'.join(chunk for chunk in chunks if chunk)
    #print(text)
    return text
  except ConnectionResetError:
    print("Connection Reset: Cannot get website data: " + url)
    return text

urlList = getListOfURLs(readCsv())

websiteDictionary = {}
startindex = 0
for i in tqdm(range(startindex, len(urlList))):
  print("Website number " + str(i))
  url = urlList[i]
  try:
    websiteDictionary[url] = getWebsiteText(url)
  except Exception as e:
    print("Cannot get all data, continue with nxt")
    print(str(e))
    continue

# write dict to file and check  
with open('websiteContent.pickle', 'wb') as handle:
  pickle.dump(websiteDictionary, handle, protocol=pickle.HIGHEST_PROTOCOL)

with open('websiteContent.pickle', 'rb') as handle:
  b = pickle.load(handle)